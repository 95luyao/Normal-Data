rnn入门的三篇经典paper。
rnn入门的三篇经典paper。带推倒的，不过看完后还是不太明白具体怎么做
越发越觉得微分的意义非常大，但模式很固定，就是一个偏导。直观上来看就是相对变化率，引申一下就是改变量，再深入就是敏感度。如果想让y对x不太敏感，那么就让y对x的变化率小点。敏感的意义又非常大，平时做表达，就是为了不让输出对输入过渡敏感，来增强鲁棒性的。一些NB paper就是从这出发搞出来的
一个问题搞了我3个多小时，找了无数解决方案，到最后是自己写错的原因。。nm
这雨下的有点大哦@小如素颜
有幸做一名智能计算外教课的助教，学的东西真不少。主要讲解基于非导数方式的优化 http://t.cn/RPPnycg
有时候的确烦躁的很，但是还是要忍着，不能把此时的脾气发给无辜的人
午餐@小如素颜
最近在学习模拟退火算法的时候，提到了深度学习中的玻尔兹曼机。原来玻尔兹曼是一种分布，利用这种机理，当该变量大于0的时候，虽然是一种随机的接受或者拒绝，但是这在理论上竟然是被证明过的可以收敛的！因为这种随机性本来就可以更大几率找到全局解。而在paper中提到的是pre-train才有全局解！！
找到了一个没有密码的wifi！哈哈
我在 @知乎 回答了【为什么基于距离的聚类算法只能发现类圆形？】：因为是在欧式空间内求的距离，对应的就是周围点到中心点的距离，只管看起来就是画圆圈啊，确定了半径，圆圈内的都是这个类了。但是这只是在欧式空间内对欧氏距离的应用，如果你对流行数据做聚类，就会发现这… http://t.cn/RPwPFqS
我在 @知乎 回答了【大数据最核心的价值是什么？】：这么多回答，这么多长篇大论的回答，个人觉得只是在举例子，没有一个简洁的回答。我借别人的话回答一下：大数据的最核心的价值是什么吧。“大数据的最核心的价值在于基于数据关联性上的预测”。也就是说大数据的核心价值就… http://t.cn/RPw7irm
饭后散步@小如素颜
纯数学推导的时候，不要带着物理意义自行取舍。或者只按照物理意义自行取舍的时候，不要又参考纯数学推导，否则会掉下很多东西。
contractive autoencoder的多出来的那个雅克比矩阵对weight和bias求导可真是麻烦呢啊，还必须要2阶了
LightTable配了半天也没再win7下弄成功，nnd
MSE中，误差分为bias和varinace两部分，但是bias和variance硬是没看懂呢，有没有学统计的大牛帮用最朴实的语言解释下呢。
用梯度下降实现一个新的模型的时候，最开心的时候就是梯度检测通过：直接意味着算法推导以及实现全部正确啦！
谁知道 matlab中如何将一个字符，比如A写入到一定大小的矩阵中，然后imshow出来这个矩阵的时候，显示的是白底黑字，黑字就是A呢？现在只有一个笨的方法就是在PS中画图，然后截取一定大小，然后再用matlab读入进来
终于可以让dropbox通过goagent来同步了。。nnd
都快10点了，怎么还这么堵！这个地方一直堵！ http://t.cn/z8p7XoL
hi@小如素颜  http://t.cn/z8AGw3f
这奇怪的天气，二环外大太阳，二环内下雨@小如素颜
正宗北方面！
@快门师
@快门师 @NassauY @会更好的-伍小舟 @什么都不知道君 @小如素颜
在sparse coding中，一般的做法都是从字典中学习到一个过完备的向量组。解释是：The advantage of having an over-complete basis is that our basis vectors are better able to capture structures and patterns inherent in the input data.为啥过完备的基就能更好的捕获输入数据的结构和模式呢
字典学习中，比如sparse coding，为何字典都是过完备的呢？
都这个点，还这么堵！ http://t.cn/RPltdyy
＃15载有我在＃   15载，有我在，建行网银15周年，共同见证成长的美好！ 15载，有我在，我的温馨历程 http://t.cn/RPTfPbb
父母没来几天，马上就要走了，好舍不得啊[悲伤][悲伤][悲伤][悲伤][思考] http://t.cn/RPd1KR6
把dropbox 换成 百度云了。。。
相关性不代表因果性，强制的把相关性当做因果性来解释的话，可能会造成很大的错误。在数据分析中一般拿到数据就用模型来计算他们的相关性，进而作为因果性来作为预测依据，结果往往预测的结果让人很诧异！
Deep learning主要用来探索数据的本质结构，寻求一个好的表达。
机器学习算法的目的是为了发现数据的本质结构，通俗的说就是尝试找到各变量之间的一个可以预测的关系，再通俗的来说，就是想搞到变量之间的一个联合分布概率。有了这个联合概率分布，自然就可以根据变量的值来计算概率了。
总是看完一篇paper后感慨颇深，是不是看的太少了，总觉得上面说的都没看到过。。。
sgd思路很简单，但是真正跑出来一个好的结果真是各方面都要考虑到，涉及到的超参太JB多了。。。准确的方法耗时间多，快速的方法耗精力多。。说到底还是缺一台性能超级NB的computer
完整的把神经网络中的tricks中的，涉及到对优化算法的筛选、对全部样本的shuffle、early stop的标准、学习率的调整、权重的初始化、sigmoid函数的选择。尼玛看完了还是不知道怎么去做，除了对优化算法的筛选，因为没给定一个学习率的初值。。没这个初值，其它的都是无用啊。。
针对一个点一个点去搞，其实很容易把这个点的原理、已经做的工作、现状 搞的差不多。前提是这个点不能太大。很多时候都是把精力搞的太分散了
theano终于在64位win8下配置ok，运行速度和CPU比起来真是杠杠的，CPU下基本控制台不变化，GPU下唰唰的。。
太高兴了哈哈 http://t.cn/RhGcFZi
今天结婚的可真多 http://t.cn/Rhqb9q6
正宗连山回锅肉 http://t.cn/z8ntsEW
这月亮是有点圆哦，咋还没到@小如素颜  http://t.cn/8kRmZX0
之前总看到用sigmoid来解释一个模型算法的时候提到saturated，一直不是很明白啥意思。今天才算是知道了，原来就是位于sigmoid函数线性的地方 就叫做饱和啊。。。
经测试，以前传言的奖学金增加 http://t.cn/8kRnoxW
重看RNN的推导，发现之前看的时候很多错误的理解
哈哈，一切顺利@MOOLO-Candy @香港夢露珠寶創始人老妖婆  http://t.cn/Rhc2gpj
矩阵写法真的很容易出错....
使用autoencoder的时候，如果隐藏层的数量小于输入的神经元个数，其实完全不必用sparsity的约束，因为这个约束是在当隐藏层的数量大于输入的时候用的。但是如果你非要用，那也没办法。。不用的目的主要是这个超餐太难选择了。
和在新加坡师兄语音了1个多小时，聊我的paper的motivation。耽误了师兄很长时间不说，聊完师兄还给我微信了一段话，说刚才的语气可能不太好，但是motivation的确存在问题，希望我不要介意。。哎，这把我说的，我怎么会介意！
早上拿到昨晚跑的结果，真是奇怪，在单纯的sparse autoencoder中，为啥decoder的weight看起来比encoder的效果要好呢？要用什么方法去分析它的原因呢
百度的云同步盘有个问题，当你一直在本地更新一个文件的时候，它理论上是会从本地同步到服务器的，结果它不是这么做的，而是从服务器往你本地同步的，造成了你t1时刻的版本，在t2时刻给你同步成为t1的。其实你在t2时刻已经做出了对t1的更改了。。真是猪一样的产品。
很奇怪，使用matlab2013的版本，利用gpuArray进行运算，发现log运算，只有在第一次的时候非常耗时。但是换成2014的版本log问题不存在，bsxfun又出问题，不知道到底什么原因，难道这个和matlab版本、显卡具体型号的实现都有关系吗
分享图片
分享图片
交叉熵原来来自波努力分布，要求数据在｛0，1｝即可。所以常见逻辑回归都是这个cost function啊
真是太诡异了，log函数怎么在CUDA下运行时间这么久，一个文件整体需要0.7秒，log一下，那怕数据量小于10，都需要0.2秒多，真是奇怪。
想了好久了都，还是无法自圆其说，还是做实验去吧。。
随便做点
之前使用sigmoid做激活函数进行BP的时候，总是很SB的把单元输入值记下来了，因为推导出来的公式是sigmoid(x)*(1-sigmoid(x))，其实sigmoid直接就是单元输出值，直接y.*(1-y)就好了。。。完全不需要保存输入值。。
今天看到deeplearning box(matlab)下的，有2个疑问：1，不知道为什么代码中对于saprsity的约束项不加入cost function中，而只是在求导的时候体现了下，这样梯度检查是肯定不通过的。2：在计算平均激活值pi的时候，为啥前面还要乘以一个0.01呢（这个0.01不是对于稀疏项的惩罚因子）
#领国庆过节费#什么？你们公司没发过节费？那还不快来微博领！万一领到1001大洋，整个假期都会感觉棒棒哒[得意地笑] http://t.cn/RhpjdBi
max函数多了个参数，一直以为matlab中max(a, 1)就是 max(a, [], 1) 一个混合代码的错误，花费了2个晚上检查出来了。还是单步调试 两份代码 两个窗口 对比发现的。。
秀下！
在结论快要形成的时候，才发现，实验一直做错的了。。前几天为了这个结论已经成功说服好几个人了都。。。
天气好
换了别人搞好的hosts文件，瞬间完全不需要任何方法，就可以顺畅打开google了啊。。
放完假，事情真是多，新事情不能耽误，旧事情也得继续跟进啊。一大早没起来就说来开会讨论RNN，幸亏一眼还是看懂了new model 的motivation和architecture，还被老板洗刷了一顿。
一个匿名函数,matlab中 f=@(x)1./(1+exp(-x))，写成了f=@(x)(1./1+exp(-x))。于是剩下的时间都去找错误去了。。
再不回家都没车了，先回家，到家继续。
公式刚刚推到完，尼玛又到12点了
看到这样的训练过程中，cost的结果，真是太开心
为了解决一些实际的问题去看paper，看完之后的感觉，好像知道怎么做了，当开始做的时候，又好像不知道具体怎么去做。
神经网络训练中对超参的选择，在实践中往往用到的是网格搜索，但是搜索前需要根据实际问题，对每个超参进行一个范围的确定。但是到底一个确定得问题怎么对应一个大概的范围呢，真是无解啊。。最近在这个上面耽误了很长时间了
不知道这样的训练，算收敛了没，训练的是一个多层的NN，使用SGD。如果不，如果不收敛，不知道接下来怎么调。。
今天是@小如素颜 最忙碌的一天，并且都不喊累，再接再厉啊！
最好的精度论文的方法就是把它背诵下来，这样就可以把每句话都弄明白，至少知道它的含义！
2个学习率在validation set上的error变化情况，蓝色基本收敛，但是绿色这个基本每次都是到了迭代3000次的时候发生震荡，不知道这到底为何，学习率设置大了，还是咋回事，我用的恒定学习率。
特征值，特征向量的意义真是博大精深，只要想办法把问题转化到矩阵，然后仔细探索其特征值和特征向量，还真是可以得到新颖的idea的！
这样来选择超参，够科学吧。一下子选择了3个超参。
batch-size在随机梯度下降with batch-size中，对于cost error的影响非常大。。
好冷啊
最近做试验发现，在MNIST上做分类，发现表达的结果越饱和，分类结果越好。不知道是数据集的问题还是啥。
看来这early stop还是要做上去，否则整个epoch跑下来太JB费时间了
使用SGD的方式，在模型上初步分析，看到了想要的一个结果。但是为了publish paper，超越他们的state-of-the-art，还是用现成的优化库来跑结果吧。
真奇怪，使用minFunc的时候，必须把前向传播，后向传播写道一个文件中吗？我为了扩展性强，把FF和BP分开写，为了满足minFunc的要求，然后给FF和BF外面加个个wrapper，然后minFunc调用wrapper，结果只迭代一次就出来了，，，
有时候只为了实现具体功能，不考虑那么多什么oo，代码写起来还是很方便。60行实现了任意层数的autoencder+denoise+weight decay+softmax。做个仿真还是问题不大。。
为啥使用LBFGS对一个8层NN训练的时候，总是迭代不了几次就收敛了呢
@小如素颜 [害羞]
追了2篇ICML的paper，主要是看它hyper-parameter咋设置的，看完知道了grid search的大概范围，不过动辄4000个单元的隐层，这机器也跑不起来啊
用了下adagrad，很奇怪，自动第二次迭代开始就收敛，仔细看了下代码，发现从第二次更新权重开始，梯度就基本全为0了。第一次用这个玩意，不知道到底咋回事，就算收敛很快，也不会第二次开始就收敛了吧
换了个系数，重试adagrad，真爽！速度比lbfgs更快，相同时间内，比之前方法跑的validation error更低！目前敢把迭代次数设置很大，然后加入early stop了，之前不敢设置过大，主要是速度太慢，时间耗不起
adagrad能跑出来这么好的效果，我满足了。在validation上的error能被优化到3，之前可都是20多啊。这是2组不同的sparsity constrained，基本结果差不多
早上来到lab，发现昨晚跑完的10组参数，这图，看着真美！
本以为adagrad很好了，结果又遇到了adadelta。。今天尝试下！
小伙伴们，快来围观！我已经升级为V6新版微博，简洁的界面带来更流畅的体验。准备好了吗？和我们一起发现新的世界吧！升级猛戳:http://t.cn/R7vgnNI http://t.cn/R7PW0Ng
adadelta的确是比adagrad更好，相同的时间，可以产生更小的validation error
MNIST数据集合，怎么就一个简单的ae+softmax就能跑出来1.78的误差，怎么跳出来的参数呢，真是NB
为了快点得到一组好的超参，还是花费2，3天把theano搞会，跑个gpu的出来
theano理解起来还是比较难，语法奇怪，可能和长期不做C有关，大多参数都和底层的优化细节有关
@小如素颜  好吃吧？
python竟然可以在函数内部定义函数，，，真是NB
真是奇怪，weight初始化为0，精度可以达到93%，如果按照均值0，方差1来初始化，精度最好是90%。。真是个坑
theano语法的确不太习惯，不过还是有规律可循。定义的时候就按照需要，在类中引入需要的变量，只管计算。再使用的时候，记得把全部的变量都传递进去就好了。有时候可能需要使用givens替代一下function中的变量。。
无意发现，我鼠标左右侧这两个按钮分别是对应浏览器的上一个网页和下一个网页。用了好几个月了都没发现，只记得给老板说买个好点的电脑跑代码用的
deeplearning.net上其实给出了很多超参的选择方法。。。按照它的做法轻松的在MNIST上实现了1.6的error。之前费了很大劲，勉强才达到1.8的error
30台iPhone6让你久等了！360手机助手3.0身边版每天5000名额抢先测 http://t.cn/R7MGKHa
@小如素颜
有时候工作效率低，是因为对工具的使用不熟悉。昨晚使用theano报了个错误，死活找不到，早上仔细的阅读每一个出错提示，加上1个配置项，然后唰唰的把错误的那行直接显示出来了。。。错误于是也就找到了
第一行是我今天写的代码，下面一行是之前写的。发现结果异常，一句话一句话对比，采用二分查找方法，结果使用二分查找的最差的时间复杂度，还是把错误给找出来了
带有early stop的离散随机梯度下降和L2正则化很相似。使用连续输入的时候，为了完美重构，一个具有一个隐藏层的autoencoder，这个隐藏层使用非线性变换，它在encoder的时候需要很小的权重，才能带来让输入在线性区域上达到线性可分，和大的权重在第二层上。 谁能解释这为啥笑权重带来线性区域可分
这paper越看越奇怪，autoencoder中之所有pre-train有效，完全是因为pre-train的时候，由于一些正则项和训练方法导致的，让解陷入了局部最优，此时正好对应一个针对于原始输入数据的一个好的transformation，这个好的transformation，在fine-tune的时候，让整个网络有一个比较好的起点
有些时候可以直接在python下做计算，使用numpy的话也可以直接做计算，2者到底啥区别呢
adadelta真是个好东西，不需要初始化学习率，实现好代码就行了。和一个筛选好之后的固定学习率对比，最终在测试集上的精度基本一致（差了百分之0.02）。只是adadelta对batch size很敏感。尽量不要设置太大。可以更频繁的validate。
coding能力还是不够，有些想法竟然无法去实现，这也不是很难啊 哎
使用theano，切记涉及到需要在外面计算的，比如输入数据，比如target等这些，他们仅仅只是个符号，在外面计算的时候，你可以替换掉这个符号的。比如一个类中，你计算计算训练误差，也要计算测试误差，传递进去的训练数据也好，测试数据也好只需要一个x即可！具体x代表意思，在外面替换
写了一天的rubbish代码！！！
gogotester配合下goagent真是无敌啦
softmax 和logistic regression的cost function还是不完全一样。。
很多时候，使用神经网络的时候，我们都强调的是它具有非线性的功能，但是一个 saprse autoencoder（使用sigmoid函数作为激活函数），和一个只是通过单纯的sparse约束（没涉及到任何非线性）学到的表达竟然是差不多的。那非线性到底还有必要吗
一个autoencder中，隐藏层单元使用非线性函数，输出层使用线性函数。输入数据是连续的，为了重构输入，在隐藏层需要小的权重，来达到让隐藏层单元处于他们的线性区域（linear regime），在输出层就需要打的权重。 为什么呢？这个处于线性区域是什么目的呢？
忽略了一个问题：3389连接过去的，是无法使用远程主机的显卡的。我还以为是我CUDA配置出错了呢，又耗费了大半天。
pre-train出来一个看起来这么差的filter，竟然都能达到最终很高的分类效果。但是一个看起来很nice的filter，竟然分类效果也没那么好（其余条件都相同的）。这个怎么解释呢
对于一个autoencoder，如果隐藏层的输出是一好的表达的话，可视化出来的对应的权重应该是线条，可以推断出来，对于一个隐藏层单元，其权重只有个别值比较大，其余值都比较小（0或者负值），这点类似dropout的思想了吧？如果让隐藏层输出值只有个别较大，同样可以让权重具备上述性质，这是sparse AE了吧
在UFLDL中说到，训练好一个autoencoder之后，能够让隐藏层的输出值最大的输入，就是隐藏层在找的特征。 始终不明白 这两者到底有何关系
之前在matlab下用GPU，一直用double去跑，发现无效果，经别人提示，修正，正确的数据类型在GPU下跑，发现效率还是不错的，可以看下图对比，都是运行同样的程序，迭代次数也一样。可以看出来GPU加速效果还是非常好非常好的，在matlab下
Leon Bottou has often suggested to pick a small number of data points (e.g. 1000) and do that many stochastic updates to evaluate a learning rate. Then make the learning rate a little smaller by dividing it by say, 2 or 3, and then do SGD on the whole dataset
使用lbfgs对MNIST，使用一个autoencoder进行优化，就可以达到cost为11左右，可视化出来效果非常好就。为什么我的sgd那么多次，选了那么多的学习率，cost曲线看起来那么的平滑，非常肯定的都认为是收敛了，怎么cost还是200多？我了个艹
autoassociator: a simple unsupervised algorithm for learning a one-layer model that computes a distributed representation for its input。这是Bengio说过的话，但是为毛，后面有人用的都是多层的的autoencoder
既然0均值之后，有利于学习的快速收敛，那为啥看了那么多的实验代码，比如MNIST的识别，都没做0均值呢?只是scale了下数据在0-1之间
可视化了一个多层nn的前两个隐藏层的特征，用mnist。发现都非常接近数字，这个到底是什么含义呢？可以明确的是：绝对不是训练不到位，因为我用weight来做的可视化，而不是输入，谁能帮解释
多层NN，无监督学习，第三层（第二个隐藏层的）特征，这到底是啥意思呢，不像边缘也不像corner，用的MNIST数据集合
要想超过别人，先要赶上别人。要想超过别人的state-of-the-art，首先也要用人家的方法可以达到人家的效果，然后再用自己的方法超过别人。下图是对第一个隐层进行可视化的结果，第一是用的UFLDL自带，其余分别为MA算法，分别采用约束范数和归一化，可以看出来约束范数的做法，还是很完美的。。
英语听力怎么这么难？？一定要征服！！
The mathematics of stochastic gradient descent are amazingly independent of the training set size. In particular, the asymptotic SGD convergence rates are independent from the sample size
SGD真的很难调很难调。。
Paper上说 直接使用随机梯度下降对使用了L1正则项的cost function，是不起作用的。为什么呢？
遇到好的经典模型，赶快掌握它，结合当前最流行的做法，结合一下，试验一下，很有可能真的就是很好的工作。scholar了一下，14年出了不少好的文章，都是这么做的，幸亏还有几个好的idea，还没人做，抓紧了，时间就是all
ZCA之后的数据（保留了90%），没经过zca的数据，经过5个epoch的学习，明显看起来效果差别很大。
尝试了固定学习率，分别是4,8，10,12。发现只有前100次迭代稍微不太一样，大于100次迭代之后，基本都是收敛，只是收敛的地方的cost不太一样，但是差别也不大，很想知道，学习率变化这么大，为什么最终收敛的地方都差不多，是学习率还是设置的小，还是这个问题有什么特性呢？
早上7点半，内急去图书馆。发现阅览室快坐满了，貌似本科生居多。这么努力，怎么可能没有breakthrough？自己好惭愧，早上难起早，其实也就两分钟的事，起来了，也就起来了，两分钟决定是否一个好的开端！ http://t.cn/8kREwiY
之前总是以无监督的时候的cost为评价目标，来衡量超参的好坏，突然想起来，无监督的cost其实对无监督的好坏并不能起到决定作用，只能是一个参考。真正能衡量好坏的应该是再加一个监督层，看最终的监督任务的精度，不是之前超参筛选出来的坏，而是筛选方案就搞错了。。
数据做了ZCA，使用SGD真的是很容易训练了
对softmax进行优化的时候发现，使用太小的固定步长学习率，反而不利于收敛，使用太大的也同样不行，正好就是那个范围内的最好。可以想象softmax的cost肯定是一个局部最小值非常多，好比一个人的皮肤，远看很光滑，近看都是坑
之前是筛选很多参数都找不到那个看起来不错的特征，现在是随便几组参数，都能跑出来看起来不错的特征。。。
有人用matlab实现过LBFGS的吗，找一个改成支持gpuArray就行了。不知道加速有多快
有一个很迷惑的问题，在sparse autoencoder中，给隐藏层输出值增加sparsity的约数，意在让输出不是大多数都有值，但是这样可能就导致一个直接约束带来的问题，就是输出值很多都是0，小部分是1，也就是基本都是处于饱和状态，而主流paper都说的是让输出值在非饱和状态才具有可区分性，2者矛盾的呢
做了一个autoencoder在学习过程中的对filter的可视化过程，一共迭代了3000次，基本初现特征，可以看到在无监督学习中，慢慢的。。。
训练了一个autoencoder，每一次迭代优化中，对隐层特征做了可视化，一共3000次迭代，基本训练出来了看起来不错的特征，可以说这也是大脑V1区第一次接受外来物体时候，识别的特征。可以看看每次学习中是个啥情况，顺便请教下：SGD到底怎么调参数，才能得到不错的结果呢，激活函数、隐藏单元数，L2，L1？
@小如素颜 http://t.cn/RzZXGzT
恼火啊真恼火，再实验做不出来，我就不发微博了
代码写不好，research真的是做不好！
真是太难写了，1000字的学习计划，写了两天还没超过750个
看到没迭代一次由17秒缩减到3秒，真是太开心[哈哈]。不过结果还是没跑出来，还是很伤心[可怜][可怜]
虽然训练出来了一个过拟合，还是很兴奋。因为之前都是超级欠拟合。避免过拟合，很容易做到，而过分欠拟合，有时候真的很无助。。
我刚换了“风轻云淡”套装，好漂亮，你们都快来试试！ http://weibo.com/home?skinId=skin049
不明白为啥在python平台下，这么多好的选择超参的lib，而在matlab下，没有！
有个疑问：看到很多地方都是先对数据进行0均值，然后再进行方差为1的操作，但是这之后数据范围基本都不在0,1之间了，而接下来还是用logistic函数来拟合，这样不是反而误差更大了吗
老婆，晚安@小如素颜 [馋嘴] http://t.cn/RvmBsCM
The reconstruction error is actually a very poor measure of the progress of learning --Hinton
有多个超参的模型真是烦人，一个3层sparse autoencoder外加一层softmax，隐层单元数长时了4组，expectable sparsity长时了2组，但是它的penalty尝试了5组，一共640组参数，用了GPU加速，跑了基本一天一夜了，gird search才进行了 三分之一
悲催的事情莫过于，计算好了的27个小时可以跑完这组参数，结果今天兴高采烈跑到实验室，发现电脑昨天晚上17点58的时候自动休眠了。。
#十年账单日记# 每一份账单都是我的日记。十年，三亿人的账单算得清，美好的改变算不清。@支付宝http://t.cn/RzKKnhj
在MNIST以及variants to MNIST上做了大量的超参选择，使用grid search。这些数据集合上普遍使用的expectable sparsity的值是0.1、也是UFLDL中给出的使用值，我测试了0.05到0.7之间的全部的数
Debug了大半天，学习率取大了，由0.01变为0.001就一切ok了。没有intuition，一下子看不出来问题所在。。[可怜]
#十年账单日记# 谁说花钱不是一种能力！我在支付宝竟然花了424,012.83元！@支付宝http://t.cn/RzKKnhj
有时候很奇怪，weight总是被蹦到一个很大的值
本想去AMAZON上买个GPU主机来跑几天程序，一看，我靠这么贵，便宜的也要一小时1.5刀，这一个月下来得多少钱了啊。。
无意中看了个matlab 的doc，终于知道 y by factor of x 的意思了，其实就是个数字，但是具体y和这个数字的关系，还是要看y前面的动词是啥
有个数据集的结果，跑过了一个state of the art。哈哈，dl果真是需要暴力的做法啊。搜索遍参数，绝对能找到个好的accuracy http://t.cn/z8A5G8w
谁知道 在学术团体任职中的president 和chair到底啥区别啊
谁知道 在学术团体任职中的president 和chair到底啥区别啊
老婆做的：日式咖喱猪排饭@小如素颜 http://t.cn/RvmBsCM
#扇贝打卡# 第 7 天  http://t.cn/RzQezPR
#扇贝打卡# 第 7 天  http://t.cn/RzQezPR
坑爹的#有道词典#，加入背诵的单词给出的音标是美式的，读音是英式的。有时候两者差别只是重低音的位置，导致我这个本来就在学习中的新手来说，真的误导我不少！
刚才和老婆闲扯，老婆斩钉截铁的说每个月手机上网可以看到上行流量和下行流量，我问什么是上行，什么是下行，她一脸正经的回答：手机上网就是上行，看图片就是下行啊。我说：以后不要给别人说你老公是学计算机的。。。
天气不错 http://t.cn/RhOLmIy
老婆给我炸油条！但是怎么看着像饼干@小如素颜 http://t.cn/RzgLHrm
明明前阵子在arxiv上看到了一篇关于autoencoder+RNN的paper，怎么今天就是找不到呢
ICLR真是个不错的会议，已经accept的论文个人觉得大多都很有价值
我的忠实铁杆Fans是: @Econymous(97分) @土豆De表哥(97分) @小如素颜(95分) @鲁东东胖(94分) @hbyido(93分) @Hyperddr(88分) @Kevin_机器学习_CA(87分) @老-木(85分) @砰砰的小屋(84分) @北北的LINDA(82分) @Twfno壁虎(80分) @三面不合Lisa(80分) 。去看你的铁杆Fans>>http://t.cn/RhNVIhz
the hidden units differentiate into categorical-units, each of which represents an input prototype with a well-defined class; and part-units representing deformations of these prototypes。这个说法相当有意思啊
之前总是把RNN拿来说可以记住长时记忆来对比传统前馈神经网络。现在看到有人说RNN的参数要少，这也的确是一大优点。。
八辆车连续追尾！ http://t.cn/RPbIDQb
过冬至啦 http://t.cn/Rzem2AD
Hidden units when encoders finished learning to representation differentiate into class units and part units.这看起来真是有点A=B+C的结果，通过对C加以一定的约束，就变成了part units，剩下的就是class units。正如sparse coding一样。这种解释真是fascinating(这句话是Bengio说的)
费劲周折注册了arxiv，取得作者的email。邮件写完的时候，发现这个问题搞懂了。。。
深度网络经过多层次学习之后得到的特征足够具备区分性，用于接下来的分类任务。但是这个具备可区分性的特征中，肯定还包含着一些所谓的“噪声”来影响接下来的分类精度，那么就需要一个机制来不选择夹杂在特征中的噪声，使用剩下的数据来进行分类任务。（个人猜测是这样子）
#扇贝打卡# 第 8 天 哈 http://t.cn/8sggATz
给一发了ICLR的作者发了封邮件，询问了下他关于RNN的工作和DL的一些异同点的疑问，作者竟然不知道DL是啥。。我勒个去。。真是醉了
今天老板喊我去替他开一个会，老板说，这个会要选主席，又不让他当，他就不去了，让我替他，选主席的时候，让我看哪个顺眼，就选哪个。结果：老板被选上主席了。。。 http://t.cn/Rzst971
太偏数学的真是啃不动。。。
Sparse coding is a class of unsupervised method learning a sets of over-complete bases to efficiently represent data. The advantage of having an over-complete bias is that these bias vectors can better capture structures and patterns inherent in the input data.后半句从来没懂过...
这次比较成功！一定要发酵好 http://t.cn/Rzem2AD
Lasso is useful for feature selection, while Ridge usually just acts as regularization.
